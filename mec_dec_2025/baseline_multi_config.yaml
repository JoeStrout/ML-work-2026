# Configuration for baseline multi-class semantic segmentation (no pre-training)

# Model - no pretrained checkpoint for baseline
num_classes: 12
num_coarse_groups: 5
resume_checkpoint: null

# Data
patch_size: 128           # Size of training patches
train_samples: 5000       # Number of training samples per epoch
val_samples: 500          # Number of validation samples per epoch

# Training
bs: 32                    # Batch size
epochs: 1501              # Number of training epochs (match LeJEPA version)
lr: 0.001                 # Learning rate

# Loss
lambda_coarse: 0.2        # Weight for coarse loss (fine loss weight is implicitly 1.0)

# Evaluation
eval_every: 5             # Evaluate every N epochs
save_every: 50            # Save checkpoint every N epochs
visualize_every: 50       # Save visualization every N epochs
