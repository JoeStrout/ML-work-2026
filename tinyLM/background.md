# Tiny Language Models: Background Literature Review

## Introduction

This document surveys the emerging field of **tiny language models** (TLMs)—language models with roughly 10M to 3B parameters that can produce coherent, grammatical text. The field has exploded since 2023, driven by the surprising discovery that with the right training data, models orders of magnitude smaller than GPT-3 can exhibit coherent language generation and even emergent reasoning capabilities.

---

## 1. The Foundational Paper: TinyStories (2023)

### Citation
Eldan, R., & Li, Y. (2023). *TinyStories: How Small Can Language Models Be and Still Speak Coherent English?* [arXiv:2305.07759](https://arxiv.org/abs/2305.07759)

### Key Insight
Prior to this work, conventional wisdom held that coherent English generation required models with at least 125M parameters (like GPT-2 small), and even then results were inconsistent. The TinyStories paper demonstrated that **models below 10 million parameters**—or even with just a single transformer block—can produce fluent, multi-paragraph stories with near-perfect grammar.

### Methodology
- **Dataset**: Synthetic short stories using vocabulary appropriate for 3-4 year olds, generated by GPT-3.5 and GPT-4
- **Vocabulary**: ~1,500 basic words
- **Evaluation**: Novel GPT-4-based grading system that scores generated text on grammar, creativity, consistency, and coherence

### Key Findings
1. **Data quality > model size**: Simple, comprehensible training data enables smaller models to exhibit coherent behavior
2. **Hierarchical capability emergence**: Language capabilities develop in stages—grammar first, then contextual consistency, then creative generation
3. **Architecture matters**: Model width correlates with knowledge retention; depth correlates with contextual understanding

### Impact
The paper has received **357+ citations** and spawned significant follow-up research, including multilingual extensions (Regional Tiny Stories for Hindi, Marathi, Bengali, Arabic) and the broader BabyLM research community.

---

## 1.1 SimpleStories: Improved Synthetic Data for Tiny Models (2025)

### Citation
Finke, L., Sreedhara, C., Dooms, T., et al. (2025). *Parameterized Synthetic Text Generation with SimpleStories.* [arXiv:2504.09184](https://arxiv.org/abs/2504.09184) (NeurIPS 2025 Poster)

### Motivation
While TinyStories enabled breakthrough research, it has limitations:
- **Formulaic structure**: 59% of stories begin with "Once upon a time"
- **Semantic restriction**: Limited to children's story themes
- **No labels**: Generation parameters not preserved for interpretability research

### Key Innovation: Parameterized Prompts
SimpleStories introduces **multi-level prompt parameterization** to control story characteristics while inducing diversity:

| Parameter | Description | Coverage |
|-----------|-------------|----------|
| Initial constraint | Part of speech + first letter | 100% |
| Topic | Subject matter | 100% |
| Theme | Underlying message | 100% |
| Writing style | Narrative voice | 100% |
| Narrative feature | Structural element | 100% |
| Grammar features | Specific constructions | 50% |
| Author persona | Writing personality | 33% |

### Dataset
- **Size**: 2 million stories each in English and Japanese
- **Readability**: Flesch-Kincaid grade 3.08±1.24 (vs. TinyStories 2.55±1.49)—slightly harder but still simple
- **Diversity**: Improved 4-gram Zipf distribution; better semantic variety per GPT-4o-mini evaluation
- **Labels preserved**: All generation parameters stored for interpretability research

### Key Findings
1. **Better sample efficiency**: Models trained on SimpleStories learn more effectively than TinyStories-trained models
2. **Improved interpretability**: Preserved labels enable studying how models encode specific features
3. **Smallest grammatical model frontier**: Authors push the boundary on minimum viable model size for coherent output
4. **Recommended over TinyStories**: Authors explicitly recommend SimpleStories for training small language models

### Evaluation Metrics
- **Syntactic diversity**: 4-gram frequency analysis using Zipf distribution
- **Semantic diversity**: GPT-4o-mini judge scoring simplicity, content diversity, style diversity
- **Label recoverability**: Topic accuracy 0.49; theme/style/narrative 0.14–0.23

### Resources
- **Paper**: [arXiv:2504.09184](https://arxiv.org/abs/2504.09184)
- **Code**: [GitHub - simple_stories_generate](https://github.com/simple-stories/simple_stories_generate)
- **Venue**: NeurIPS 2025 Poster

---

## 2. The BabyLM Challenge: Cognitively-Inspired Small Models

### Overview
The [BabyLM Challenge](https://babylm.github.io/) is a community effort to close the data-efficiency gap between human and computational language learners. Children acquire language from less than 100 million words; LLMs typically require 3-4 orders of magnitude more data.

### Challenge Structure
- **10M-word track**: Text-only, extreme data constraint
- **100M-word track**: Text-only, developmentally plausible
- **Multimodal track**: 100M words + images
- **New for 2025**: INTERACTION track (multi-agent training) + compute limits (max 10 epochs)

### Key Results from 2024
- **31 submissions** with diverse methodologies
- **Best approach**: Hybrid causal-masked language model (combining GPT and BERT objectives)
- **Curriculum learning** emerged as a popular strategy—training on simple text first, progressing to complex text

### Research Insight
> "Models trained on child-scale data using tailored objectives and architecture can approach, or in some settings exceed, models trained on billions of tokens."

**Links**: [BabyLM Official Site](https://babylm.github.io/) | [2024 Findings Paper](https://arxiv.org/abs/2412.05149)

---

## 3. Microsoft's Phi Series: "Textbooks Are All You Need"

Microsoft Research demonstrated that **data quality** can substitute for data quantity and model size.

### Phi-1 (June 2023)
- **Size**: 1.3B parameters
- **Training**: 8 A100s for 4 days on ~7B tokens total
- **Data**: "Textbook quality" web data + GPT-3.5-generated synthetic textbooks
- **Result**: Beats models 10× larger on code benchmarks (HumanEval, MBPP)

### Phi-1.5 (September 2023)
- **Size**: 1.3B parameters
- **Training data**: Phi-1 data + 20B tokens of synthetic NLP "textbook" content
- **Focus**: Common sense reasoning and general knowledge

### Phi-2 (December 2023)
- **Size**: 2.7B parameters
- **Training**: 14 days on 96 A100s, 1.4T tokens (multiple passes)
- **Performance**: Matches or outperforms models up to 25× larger on complex benchmarks

### Key Papers
- [Textbooks Are All You Need](https://huggingface.co/papers/2306.11644)
- [Phi-2 Blog Post](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)

---

## 4. Alternative Architectures for Efficient Small Models

### 4.1 Mamba: Selective State Space Models

**Paper**: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) (December 2023)

Mamba is the first competitive alternative to transformers for autoregressive language modeling.

**Key Benefits**:
- **5× higher throughput** than transformers at inference
- **Linear scaling** with sequence length (vs. quadratic for attention)
- **No KV cache** needed, enabling higher batch sizes
- Performance scales to million-length sequences

**How it works**: Uses a selective scan algorithm that filters relevant information dynamically, combined with hardware-aware parallel computation.

**Mamba-2 (2024)**: 2-8× faster than Mamba-1, competitive with transformers on language modeling. [Paper](https://arxiv.org/abs/2405.21060)

**Industry adoption**: Mistral's Codestral Mamba, AI21's Jamba, IBM Granite 4.0 (hybrid architectures)

### 4.2 RWKV: RNNs Reinvented

**Paper**: [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048) (EMNLP 2023)

RWKV combines transformer-style parallel training with RNN-style efficient inference.

**Key Features**:
- **O(n) complexity** vs. O(n²) for transformers
- **Constant memory** (no KV cache)
- **Infinite context length** potential
- Parallelizable during training, recurrent during inference

**Architecture Evolution**:
- RWKV-4: First to prove RNN-transformer fusion works (scaled to 14B parameters)
- RWKV-5/6 "Eagle/Finch": Matrix-valued representations
- **RWKV-7 "Goose"**: Dynamic state evolution, surpasses transformers in efficiency and performance

**Links**: [GitHub](https://github.com/BlinkDL/RWKV-LM) | [Official Site](https://www.rwkv.com/)

### 4.3 Gated Attention (NeurIPS 2025 Best Paper)

**Paper**: [Gated Attention for Large Language Models](https://arxiv.org/abs/2505.06708)

Adding gating after attention provides:
- **Non-linearity**: Crucial transformations after low-rank softmax mapping
- **Sparsity**: Query-dependent sparse gating creates focused attention
- **Eliminates attention sinks**: No more forcing attention mass onto irrelevant tokens

**Results**: -0.265 perplexity, +2.03 MMLU points. Already integrated into Qwen3-Next.

### 4.4 Gated Linear Attention (GLA)

Competitive with LLaMA-architecture transformers while enabling linear-time inference. FlashLinearAttention implementation is faster than FlashAttention-2 even at short (1K) sequence lengths.

---

## 5. Production-Ready Small Model Families

### 5.1 TinyLlama (1.1B)

**Paper**: [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385) (January 2024)

- **Architecture**: Llama 2 compatible
- **Training**: 3 trillion tokens over 90 days on 16 A100-40G GPUs
- **Data mix**: SlimPajama (natural language) + Starcoderdata (code), ~7:3 ratio
- **Optimizations**: Flash Attention 2, fused operations → 56% MFU without activation checkpointing
- **Quantized size**: 637 MB (4-bit)

**Use case**: Edge deployment, resource-constrained environments

### 5.2 SmolLM Series (Hugging Face)

**SmolLM** (July 2024): 135M, 360M, 1.7B parameter models
- **Training data**: SmolLM-Corpus (Cosmopedia v2 synthetic textbooks, Python-Edu, FineWeb-Edu)
- **Architecture innovations**: GQA, depth over width for smaller variants

**SmolLM2** (February 2025): Overtrained on 11T tokens with staged curriculum
- New datasets: FineMath, Stack-Edu, SmolTalk

**SmolLM3** (2025): 3B parameters, dual-mode reasoning, 6 languages, long context
- Pretrained on 11.2T tokens with NoPE (no positional encoding) at 3:1 ratio

**Resources**: [SmolLM Blog](https://huggingface.co/blog/smollm) | [Smol Training Playbook](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook) | [Smol Course](https://github.com/huggingface/smol-course)

### 5.3 Qwen Small Models (Alibaba)

**Qwen2.5** (September 2024):
- Sizes: 0.5B, 1.5B, 3B (plus larger variants)
- Training: 18 trillion tokens
- **Qwen2.5-0.5B outperforms Gemma2-2.6B** on math and coding tasks

**Qwen3** (April 2025):
- Sizes: 0.6B, 1.7B (plus larger)
- Training: 36 trillion tokens, 119 languages
- Strong-to-Weak distillation enables edge-side models to outperform larger baselines

---

## 6. Educational Resources & Tools

### 6.1 nanoGPT (Andrej Karpathy)

The definitive educational implementation for understanding transformers.

- **Code size**: ~300 lines for model, ~300 lines for training
- **Capability**: Can reproduce GPT-2 (124M) on single 8×A100 node in ~4 days
- **Minimum config**: 6-layer, 6-head transformer trains in 3 minutes on one A100

**Repositories**:
- [nanoGPT](https://github.com/karpathy/nanoGPT) (pretraining, now deprecated)
- [build-nanogpt](https://github.com/karpathy/build-nanogpt) (video course with step-by-step commits)
- **nanochat** (new): Full-stack ChatGPT clone training/inference pipeline

### 6.2 Hugging Face Smol Course

A practical, hands-on course covering:
- Training small language models from scratch
- Fine-tuning and alignment
- Production deployment

**Link**: [github.com/huggingface/smol-course](https://github.com/huggingface/smol-course)

---

## 7. Key Themes & Takeaways

### Data Quality Trumps Scale
The TinyStories and Phi papers independently demonstrate that curated, high-quality data enables dramatically smaller models. Synthetic "textbook" data generated by larger models is particularly effective.

### Architecture Innovation Continues
While transformers dominate, Mamba (SSMs) and RWKV (modern RNNs) offer compelling alternatives with linear complexity. Hybrid architectures combining attention and SSM layers are gaining traction.

### Compute Efficiency is Critical
Small models benefit enormously from:
- Flash Attention / Gated Linear Attention
- Fused operations (layernorm, cross-entropy, positional embeddings)
- Grouped Query Attention (GQA)
- Quantization (4-bit models fit in <1GB)

### Developmentally-Plausible Training Works
The BabyLM community has shown that curriculum learning and data constraints inspired by human language acquisition can produce surprisingly capable models.

---

## 8. Recommendations: Getting Up to Speed

### Week 1: Foundations
1. **Read TinyStories paper** ([arXiv](https://arxiv.org/abs/2305.07759)) - understand the core insight
2. **Work through build-nanogpt** ([GitHub](https://github.com/karpathy/build-nanogpt)) - implement a transformer from scratch
3. **Train a tiny model** on SimpleStories or TinyStories dataset using nanoGPT (SimpleStories recommended for better sample efficiency)

### Week 2: Modern Architectures
1. **Read Mamba paper** ([arXiv](https://arxiv.org/abs/2312.00752)) - understand SSMs
2. **Study RWKV architecture** ([GitHub wiki](https://wiki.rwkv.com/basic/architecture.html))
3. **Experiment** with pre-trained small models (SmolLM, TinyLlama, Qwen-0.5B)

### Week 3: Advanced Topics
1. **Explore BabyLM Challenge** papers from 2024 ([ACL Anthology](https://aclanthology.org/events/babylm-2024/))
2. **Study Phi training approach** - synthetic textbook generation
3. **Take the Hugging Face Smol Course** ([GitHub](https://github.com/huggingface/smol-course))

### Ongoing
- Follow the [BabyLM 2025 workshop](https://babylm.github.io/) (EMNLP 2025)
- Monitor Qwen and SmolLM releases for state-of-the-art small models
- Consider contributing to open-source tiny model efforts

### Hands-On Experiments
| Experiment | Hardware | Time | Goal |
|------------|----------|------|------|
| Train nanoGPT on SimpleStories | 1× consumer GPU | 1-3 hours | Understand training dynamics (better than TinyStories) |
| Fine-tune TinyLlama | 1× A100 or 4-bit on consumer GPU | 2-4 hours | Learn fine-tuning workflow |
| Train SmolLM-135M from scratch | 1× A100 | 1-2 days | Full pretraining experience |
| BabyLM challenge dataset | Any | Variable | Try curriculum learning |

---

## 9. Key Papers Bibliography

| Paper | Year | Key Contribution |
|-------|------|------------------|
| [TinyStories](https://arxiv.org/abs/2305.07759) | 2023 | Coherent generation from <10M params |
| [SimpleStories](https://arxiv.org/abs/2504.09184) | 2025 | Parameterized synthetic data, improved over TinyStories |
| [Textbooks Are All You Need](https://huggingface.co/papers/2306.11644) | 2023 | Synthetic textbook training |
| [RWKV](https://arxiv.org/abs/2305.13048) | 2023 | RNN-transformer hybrid |
| [Mamba](https://arxiv.org/abs/2312.00752) | 2023 | Selective state space models |
| [TinyLlama](https://arxiv.org/abs/2401.02385) | 2024 | Open 1.1B model on 3T tokens |
| [Mamba-2](https://arxiv.org/abs/2405.21060) | 2024 | State space duality |
| [BabyLM 2024 Findings](https://arxiv.org/abs/2412.05149) | 2024 | Cognitively-plausible training |
| [SmolLM2](https://huggingface.co/papers/2502.02737) | 2025 | Data-centric small model training |
| [Gated Attention](https://arxiv.org/abs/2505.06708) | 2025 | Attention-sink-free transformers |

---

## 10. Useful Links

### Datasets
- [TinyStories Dataset](https://huggingface.co/datasets/roneneldan/TinyStories)
- [SimpleStories Dataset](https://github.com/simple-stories/simple_stories_generate) - Improved TinyStories alternative with labels
- [SmolLM-Corpus](https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus)
- [BabyLM Training Data](https://babylm.github.io/)

### Pre-trained Models
- [SmolLM Models](https://huggingface.co/HuggingFaceTB)
- [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [Qwen2.5-0.5B/1.5B](https://huggingface.co/Qwen)
- [Microsoft Phi-2](https://huggingface.co/microsoft/phi-2)

### Code & Tutorials
- [nanoGPT](https://github.com/karpathy/nanoGPT)
- [build-nanogpt](https://github.com/karpathy/build-nanogpt)
- [Hugging Face Smol Course](https://github.com/huggingface/smol-course)
- [RWKV-LM](https://github.com/BlinkDL/RWKV-LM)
- [Mamba](https://github.com/state-spaces/mamba)
